{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86d5b1-f232-4844-9192-716f5563015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 9/13/2022 G CodeLab: Intro to Vertex Pipelines\n",
    "#origin: G Community event 9/12/2022 MLOps with Skander\n",
    "#refer to https://codelabs.developers.google.com/vertex-pipelines-intro?hl=en \n",
    "#Written by Sara Robinson, Last updated Jul 18, 2022\n",
    "\n",
    "#infra: Qwiklabs -> Vertex AI\n",
    "# User-managed nb ->  TensorFlow Enterprise 2.3 (with LTS) instance type without GPUs\n",
    "\n",
    "#history\n",
    "# 9/13/2022 CODELAB\n",
    "#      Refer to full Codelab at\n",
    "#  https://onedrive.live.com/redir?resid=838B6473B6EE9788%211476&page=Edit&wd=target%28GCommunity_202206.one%7C5c4c7ff9-c092-42dc-abfd-991b638f83fa%2FMLOps%20Codelab%20Intro%20to%20Vertex%20Pipelines%7C4b48ed42-71a1-43fb-bda7-81b5b80ed28e%2F%29&wdorigin=703"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf202a04-552e-420c-a1a5-e937ac37c583",
   "metadata": {},
   "source": [
    "# Intro to Vertex Pipelines\n",
    "\n",
    "In this lab, you will learn how to create and run ML pipelines with Vertex Pipelines.\n",
    "\n",
    "What you learn\n",
    "You'll learn how to:\n",
    "\n",
    "- Use the Kubeflow Pipelines SDK to build scalable ML pipelines  \n",
    "- Create and run a 3-step intro pipeline that takes text input  \n",
    "- Create and run a pipeline that trains, evaluates, and deploys an AutoML classification model  \n",
    "- Use pre-built components for interacting with Vertex AI services, provided through the google_cloud_pipeline_components library  \n",
    "- Schedule a pipeline job with Cloud Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec75ac0-3b37-403f-a5a8-b80cf1f7c62e",
   "metadata": {},
   "source": [
    "## 4. Vertex Pipelines setup\n",
    "\n",
    "### Step 1: Create Python notebook and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666060e4-8438-4767-ba60-b667dfaf3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f55d658-4dc4-498d-81fb-9bd1fe1fd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform==1.7.0 in ./.local/lib/python3.7/site-packages (1.7.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (2.34.4)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in ./.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.44.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.20.6)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (2.8.1)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (3.19.4)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in ./.local/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.56.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.28.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.47.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.48.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.3.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.32.0->google-cloud-aiplatform==1.7.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.7.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (59.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (1.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.26.11)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (1.15.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.21)\n",
      "Requirement already satisfied: kfp==1.8.9 in ./.local/lib/python3.7/site-packages (1.8.9)\n",
      "Collecting google-cloud-pipeline-components==0.2.0\n",
      "  Downloading google_cloud_pipeline_components-0.2.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-toolbelt<1,>=0.8.0 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.9.1)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (3.2.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (2.1.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.2.13)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (3.0.1)\n",
      "Requirement already satisfied: absl-py<=0.11,>=0.9 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.11.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.12.11)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.4.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (5.4.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (18.20.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.15)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.44.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (0.8.10)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (3.19.4)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (8.1.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.35.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.13 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.1.16)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.1.10)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (1.9.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.8.5)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in ./.local/lib/python3.7/site-packages (from kfp==1.8.9) (3.10.0.2)\n",
      "Collecting google-cloud-notebooks>=0.4.0\n",
      "  Downloading google_cloud_notebooks-1.4.2-py2.py3-none-any.whl (355 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.3/355.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-api-core<2dev,>=1.26.0\n",
      "  Downloading google_api_core-1.33.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform>=1.4.3 in ./.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.2.0) (1.7.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.9) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp==1.8.9) (4.11.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.9) (1.14.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.9) (1.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.56.4)\n",
      "Collecting protobuf<4,>=3.13.0\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9) (0.20.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (4.2.4)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (59.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (0.2.7)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (2.34.4)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (2.8.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (1.20.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (21.3)\n",
      "Collecting proto-plus>=1.10.1\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.3.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (1.26.11)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (2022.6.15)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9) (1.3.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.9) (0.37.1)\n",
      "Collecting google-api-core[grpc]<3.0.0dev,>=1.26.0\n",
      "  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.48.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.47.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.9) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.9) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp==1.8.9) (3.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.9) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.21)\n",
      "Installing collected packages: protobuf, proto-plus, google-api-core, google-cloud-notebooks, google-cloud-pipeline-components\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.26.3 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "tfx 0.26.3 requires attrs<21,>=19.3.0, but you have attrs 22.1.0 which is incompatible.\n",
      "tfx 0.26.3 requires click<8,>=7, but you have click 8.1.3 which is incompatible.\n",
      "tfx 0.26.3 requires docker<5,>=4.1, but you have docker 5.0.3 which is incompatible.\n",
      "tfx 0.26.3 requires kubernetes<12,>=10.0.1, but you have kubernetes 18.20.0 which is incompatible.\n",
      "tfx 0.26.3 requires pyarrow<0.18,>=0.17, but you have pyarrow 9.0.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 9.0.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires pyarrow<0.18,>=0.17, but you have pyarrow 9.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 9.0.0 which is incompatible.\n",
      "tensorflow-metadata 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 9.0.0 which is incompatible.\n",
      "ml-pipelines-sdk 0.26.3 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "ml-pipelines-sdk 0.26.3 requires docker<5,>=4.1, but you have docker 5.0.3 which is incompatible.\n",
      "ml-metadata 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 0.11.0 which is incompatible.\n",
      "ml-metadata 0.26.0 requires attrs<21,>=20.3, but you have attrs 22.1.0 which is incompatible.\n",
      "google-cloud-pubsublite 1.4.2 requires google-cloud-pubsub<3.0.0dev,>=2.1.0, but you have google-cloud-pubsub 1.7.2 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\n",
      "apache-beam 2.28.0 requires httplib2<0.18.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\n",
      "apache-beam 2.28.0 requires pyarrow<3.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\n",
      "apache-beam 2.28.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-1.33.0 google-cloud-notebooks-1.4.2 google-cloud-pipeline-components-0.2.0 proto-plus-1.22.1 protobuf-3.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.7.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp==1.8.9 google-cloud-pipeline-components==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb11f8e-6809-43dc-ae50-3ba7f9ed2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6998376-091d-4608-a680-ee8f346c9987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.9\n",
      "google_cloud_pipeline_components version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31606444-33c7-4c31-98ad-6a51d4d5541f",
   "metadata": {},
   "source": [
    "### Step 2: Set your project ID and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0308109-bf20-4d32-a3fb-2615636d44d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  qwiklabs-gcp-02-3b82046ef315\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d8ccda-d444-4bc7-bdb0-3ca39f14130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d549f4-5b0e-466c-ae0c-76692e171a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22494cf9-f82b-40c6-9ef0-f69c74f3bd35",
   "metadata": {},
   "source": [
    "### Step 3: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e956f-00ae-4287-b069-174fd4ee63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f059e-44df-4084-a526-bc541e3034af",
   "metadata": {},
   "source": [
    "### Step 4: Define constants\n",
    "`PIPELINE_ROOT` is the Cloud Storage path where the artifacts created by our pipeline will be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bba00d-9254-43d4-abdb-7a7b6a31d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://qwiklabs-gcp-02-3b82046ef315-bucket/pipeline_root/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c264b9-dabb-4954-bc9f-0545b19ecd8a",
   "metadata": {},
   "source": [
    "## 5. Creating your first pipeline\n",
    "Create a short pipeline using the KFP SDK (this pipeline doesn't do anything ML related)\n",
    "\n",
    "- How to create custom components in the KFP SDK\n",
    "- How to run and monitor a pipeline in Vertex Pipelines\n",
    "\n",
    "### Step 1: Create a Python function based component\n",
    " 3 components in our first pipeline\n",
    " \n",
    " 1. `product_name` component, which simply takes a string as input and returns that string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1228a5e-25cb-45aa-ad31-4e252a222449",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", output_component_file=\"first-component.yaml\")\n",
    "def product_name(text: str) -> str:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f55da-6f8c-4df3-b90c-469b647addfd",
   "metadata": {},
   "source": [
    "If you wanted to share this component with someone, you could send them the generated yaml file and have them load it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1212c2d-9b5f-4f9e-aeb4-026081990a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_component = kfp.components.load_component_from_file('./first-component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb35187-4c14-40bb-93ad-d5501398d143",
   "metadata": {},
   "source": [
    "### Step 2: Create two additional components\n",
    "To complete our pipeline, we'll create two more components. \n",
    "\n",
    "2. `emoji` takes a string as input, and converts this string to its corresponding emoji if there is one. It returns a tuple with the input text passed, and the resulting emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d52d29-f151-4309-9d16-311f5c25bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"emoji\"])\n",
    "def emoji(\n",
    "    text: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"emoji_text\", str),  # Return parameters\n",
    "        (\"emoji\", str),\n",
    "    ],\n",
    "):\n",
    "    import emoji\n",
    "\n",
    "    emoji_text = text\n",
    "    emoji_str = emoji.emojize(':' + emoji_text + ':', language='alias')\n",
    "    print(\"output one: {}; output_two: {}\".format(emoji_text, emoji_str))\n",
    "    return (emoji_text, emoji_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f526-e19f-48f1-98b4-0240120a9716",
   "metadata": {},
   "source": [
    "3. `build_sentence` consume the output of the first two and combine them to return a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e64f54c2-2b73-4f69-a82e-d7c21fd00744",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def build_sentence(\n",
    "    product: str,\n",
    "    emoji: str,\n",
    "    emojitext: str\n",
    ") -> str:\n",
    "    print(\"We completed the pipeline, hooray!\")\n",
    "    end_str = product + \" is \"\n",
    "    if len(emoji) > 0:\n",
    "        end_str += emoji\n",
    "    else:\n",
    "        end_str += emojitext\n",
    "    return(end_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7e124-03b2-4304-89da-6b35988022b7",
   "metadata": {},
   "source": [
    "### Step 3: Putting the components together into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad1028-34ec-490b-bdec-997756293869",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"hello-world\",\n",
    "    description=\"An intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# You can change the `text` and `emoji_str` parameters here to update the pipeline output\n",
    "def intro_pipeline(text: str = \"Vertex Pipelines\", emoji_str: str = \"sparkles\"):\n",
    "    product_task = product_name(text)\n",
    "    emoji_task = emoji(emoji_str)\n",
    "    consumer_task = build_sentence(\n",
    "        product_task.output,\n",
    "        emoji_task.outputs[\"emoji\"],\n",
    "        emoji_task.outputs[\"emoji_text\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1efc96-8404-4389-8ad8-aa631fc62711",
   "metadata": {},
   "source": [
    "### Step 4: Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffa10d62-c9e9-45e6-aa59-81ce4b0503a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    piapeline_func=intro_pipeline, package_path=\"intro_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a6059e-04b7-4b82-bdf5-085ad41cc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8117624-fb53-43ec-a606-8f92f35e0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"hello-world-pipeline\",\n",
    "    template_path=\"intro_pipeline_job.json\",\n",
    "    job_id=\"hello-world-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6240bf-9353-49ba-bcd0-5b219815e9ec",
   "metadata": {},
   "source": [
    "Create a new pipeline execution and see logs with a link to view the pipeline run in your console  \n",
    "(5-6 minutes to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "390ec863-69a5-420a-a542-209482519426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/105288858048/locations/us-central1/pipelineJobs/hello-world-pipeline-20220913182929\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/105288858048/locations/us-central1/pipelineJobs/hello-world-pipeline-20220913182929')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hello-world-pipeline-20220913182929?project=105288858048\n"
     ]
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635ffc8-63b4-4b61-8c8d-023cbc6f1291",
   "metadata": {},
   "source": [
    "Now that you're familiar with how the KFP SDK and Vertex Pipelines works, you're ready to build a pipeline that creates and deploys an ML model using other Vertex AI services. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441038e-225e-497a-9539-491a434650b3",
   "metadata": {},
   "source": [
    "## 6. Creating an end-to-end ML pipeline\n",
    "This ML pipeline will:\n",
    "\n",
    "- Create a Dataset in Vertex AI  \n",
    "- Train a tabular classification model with AutoML  \n",
    "- Get evaluation metrics on this model  \n",
    "- Based on the evaluation metrics, decide whether to deploy the model using conditional logic in Vertex Pipelines  \n",
    "- Deploy the model to an endpoint using Vertex Prediction  \n",
    "\n",
    "Dataset (tabular):  UCI Machine Learning Dry beans dataset, from: KOKLU, M. and OZKAN, I.A., (2020), \"Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.\"In Computers and Electronics in Agriculture, 174, 105507. DOI.\n",
    "\n",
    "### Step 1: A custom component for model evaluation\n",
    "The custom component we'll define will be used towards the end of our pipeline once model training has completed. This component will do a few things:\n",
    "\n",
    "- Get the evaluation metrics from the trained AutoML classification model  \n",
    "- Parse the metrics and render them in the Vertex Pipelines UI  \n",
    "- Compare the metrics to a threshold to determine whether the model should be deployed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e816260e-2d0e-4a3a-acde-82e99d0a3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"tabular_eval_component.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classification_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"us-central1\",\n",
    "    api_endpoint: str,  # \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform as aip\n",
    "\n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(client, model_name):\n",
    "        from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is\n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    logging.info(\"{} < {}; returning False\".format(metrics_dict[k], v))\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aip.init(project=project)\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aip.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\n",
    "        client, model_resource_path\n",
    "    )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a053b-d1bb-46c7-8c42-7773c6a48bc2",
   "metadata": {},
   "source": [
    "### Step 2: Adding Google Cloud pre-built components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a6dfb-dce0-42a0-944f-5540da35ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-beans1663093982\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "DISPLAY_NAME = 'automl-beans{}'.format(str(int(time.time())))\n",
    "print(DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7dd2d7c-7822-453c-a6bf-d4a867b2c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"automl-tab-beans-training-v2\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = \"bq://aju-dev-demos.beans.beans1\",\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=display_name, bq_source=bq_source\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\",\n",
    "    )\n",
    "    model_eval_task = classification_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    with dsl.Condition(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            display_name=\"train-automl-beans\",\n",
    "        )\n",
    "\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e0b5da-e750-4068-a49f-b112375e7b35",
   "metadata": {},
   "source": [
    "### Step 3: Compile and run the end-to-end ML pipeline\n",
    "a little over an hour to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4172c7c1-78cb-4c85-9432-05f42de0bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"tab_classif_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b159116e-d08a-4f5c-8034-198982752ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"automl-tab-beans-training\",\n",
    "    template_path=\"tab_classif_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID, \"display_name\": DISPLAY_NAME},\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef2fb0b7-fce2-4dc8-a88b-2ecbdeed6cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/105288858048/locations/us-central1/pipelineJobs/automl-tab-beans-training-v2-20220913183332\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/105288858048/locations/us-central1/pipelineJobs/automl-tab-beans-training-v2-20220913183332')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-tab-beans-training-v2-20220913183332?project=105288858048\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8c044-5b85-430e-a13a-2940a59eead9",
   "metadata": {},
   "source": [
    "#### More\n",
    "View and analyze your Vertex Pipelines graph in the console\n",
    "\n",
    "Refer to https://onedrive.live.com/redir?resid=838B6473B6EE9788%211476&page=Edit&wd=target%28GCommunity_202206.one%7C5c4c7ff9-c092-42dc-abfd-991b638f83fa%2FMLOps%20Codelab%20Intro%20to%20Vertex%20Pipelines%7C4b48ed42-71a1-43fb-bda7-81b5b80ed28e%2F%29&wdorigin=703  \n",
    "Examples of:  \n",
    "- pipeline (graph)  \n",
    "- metrics -> confusion matrix  \n",
    "- lineage\n",
    "\n",
    "### Step 4: Comparing metrics across pipeline runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "138f57fa-b679-41f2-92e6-bdefeb5bb211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:gcp_region</th>\n",
       "      <th>param.input:api_endpoint</th>\n",
       "      <th>param.input:thresholds_dict_str</th>\n",
       "      <th>param.input:project</th>\n",
       "      <th>param.input:display_name</th>\n",
       "      <th>param.input:bq_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>automl-tab-beans-training-v2</td>\n",
       "      <td>automl-tab-beans-training-v2-20220913183332</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>us-central1-aiplatform.googleapis.com</td>\n",
       "      <td>{\"auRoc\": 0.95}</td>\n",
       "      <td>qwiklabs-gcp-02-3b82046ef315</td>\n",
       "      <td>automl-beans1663093982</td>\n",
       "      <td>bq://aju-dev-demos.beans.beans1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pipeline_name                                     run_name  \\\n",
       "0  automl-tab-beans-training-v2  automl-tab-beans-training-v2-20220913183332   \n",
       "\n",
       "  param.input:gcp_region               param.input:api_endpoint  \\\n",
       "0            us-central1  us-central1-aiplatform.googleapis.com   \n",
       "\n",
       "  param.input:thresholds_dict_str           param.input:project  \\\n",
       "0                 {\"auRoc\": 0.95}  qwiklabs-gcp-02-3b82046ef315   \n",
       "\n",
       "  param.input:display_name            param.input:bq_source  \n",
       "0   automl-beans1663093982  bq://aju-dev-demos.beans.beans1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=\"automl-tab-beans-training-v2\")\n",
    "small_pipeline_df = pipeline_df.head(2)\n",
    "small_pipeline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ef3bd-8d37-4324-aecf-79f2630ab3d5",
   "metadata": {},
   "source": [
    "### Done\n",
    "You've learned how to use Vertex AI to:\n",
    "\n",
    "- Use the Kubeflow Pipelines SDK to build end-to-end pipelines with custom components  \n",
    "- Run your pipelines on Vertex Pipelines and kick off pipeline runs with the SDK  \n",
    "- View and analyze your Vertex Pipelines graph in the console  \n",
    "- Use pre-built pipeline components to add Vertex AI services to your pipeline  \n",
    "- Schedule recurring pipeline jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cf27e-8cc6-44ce-a1be-57738766dc65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
