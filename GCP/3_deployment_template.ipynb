{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 8/2/2022 GCP template - VertexAI\n",
    "#template copy from MD work for Hack22\n",
    "#generic: project, bucket, BQ, dataflow, dataset, pubsub (streaming), cloud fn\n",
    "\n",
    "#$note $secrets key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q --no-warn-script-location\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "## Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## Set up your Google Cloud project\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager)\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the Vertex AI APIs, Compute Engine APIs, and Cloud Storage, Pub/Sub API, Dataflow API, Cloud Functions API, Cloud Build API.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com,pubsub.googleapis.com,dataflow.googleapis.com,cloudfunctions.googleapis.com,cloudbuild.googleapis.com)\n",
    "\n",
    "4. [The Google Cloud SDK](https://cloud.google.com/sdk) is already installed in Google Cloud Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL - DEFAULTS TO CURRENT PROJECT_ID\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "# WARNING: Do not change to us-west1 -- DataFlow is only available in us-central1\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIEW BUCKET CONTENT:\n",
    "# ! gsutil ls -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pub/Sub to email Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_2_NAME = '[your-topic]'\n",
    "TOPIC_2_URI = f'projects/{PROJECT_ID}/topics/{TOPIC_2_NAME}'\n",
    "TOPIC_2_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $TOPIC_2_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cloud Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set functions/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS_NAME = '[your_fn]'\n",
    "# RUNTIME = 'python37'\n",
    "# ! gcloud functions deploy $FUNCTIONS_NAME   --source 'gs://[your-bucket]/functions/main.py'   --runtime $RUNTIME   --trigger-topic $TOPIC_2_NAME --set-env-vars GOOGLE_FUNCTION_TARGET=hello_pubsub\n",
    "# ! gcloud functions deploy $FUNCTIONS_NAME --runtime=$RUNTIME --trigger-topic=$TOPIC_2_NAME --source='gs://[your-bucket]/functions/functions.zip' \n",
    "\n",
    "# GETTING ERRORS\n",
    "# https://cloud.google.com/functions/docs/calling/pubsub#functions_calling_pubsub-python\n",
    "# INSTALLLED FROM THE UI INSTEAD:\n",
    "# https://cloud.google.com/security-command-center/docs/how-to-enable-real-time-notifications#obtain-a-sendgrid-email-api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS IN THE EMAIL:\n",
    "\n",
    "#######\n",
    "# RUNTIME = python37\n",
    "# Region = us-central1\n",
    "# trigger = our topic\n",
    "# Entry Point = send_email\n",
    "\n",
    "####################### main.py:\n",
    "\n",
    "# import base64\n",
    "\n",
    "# def send_email(event, context):\n",
    "#     \"\"\"Triggered from a message on a Cloud Pub/Sub topic.\n",
    "#     Args:\n",
    "#          event (dict): Event payload.\n",
    "#          context (google.cloud.functions.Context): Metadata for the event.\n",
    "#     \"\"\"\n",
    "\n",
    "#     import logging\n",
    "#     from sendgrid import SendGridAPIClient\n",
    "#     from sendgrid.helpers.mail import Mail, Email\n",
    "#     from python_http_client.exceptions import HTTPError\n",
    "\n",
    "\n",
    "#     pubsub_message = base64.b64decode(event['data']).decode('utf-8')\n",
    "#     print(pubsub_message)\n",
    "\n",
    "\n",
    "\n",
    "#     log = logging.getLogger(__name__)\n",
    "\n",
    "#     SENDGRID_API_KEY = 'your-key' #$secrets key\n",
    "#     sg = SendGridAPIClient(SENDGRID_API_KEY)\n",
    "\t\n",
    "   \n",
    "#     html_content = f\"\"\"\n",
    "    \n",
    "#     ANOMALY: \n",
    "    \n",
    "#     {pubsub_message}\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     message = Mail(\n",
    "#         to_emails=\"[your-email]@[your-domain].com\",\n",
    "#         from_email=Email('[your-email]@[your-domain].com', \"Notifications User\"),\n",
    "#         subject=f\"Anomaly Detected\",\n",
    "#         html_content=html_content\n",
    "#         )\n",
    "\n",
    "#     try:\n",
    "#       print('Sending email...')\n",
    "#       response = sg.send(message)\n",
    "        \n",
    "#       return f\"email.status_code={response.status_code}\"\n",
    "#     except HTTPError as e:\n",
    "#         return e\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "####################### requirements.txt:\n",
    "\n",
    "# # https://pypi.org/project/sendgrid/\n",
    "# sendgrid==6.0.5\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO Prediction and Anomaly to Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOCATION = f\"{BUCKET_URI}/training/models/energy.pkl\"\n",
    "model_file = 'training/models/model.pkl'\n",
    "! gsutil cp -r $MODEL_LOCATION $model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "MODEL = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "\n",
    "def check_anomaly(actual, predicted, treshhold):\n",
    "    lower = predicted-treshhold\n",
    "    upper = predicted+treshhold\n",
    "    if actual > upper:\n",
    "        anomaly = 1\n",
    "    elif actual < lower:\n",
    "        anomaly = 1\n",
    "    else:\n",
    "        anomaly = 1\n",
    "    return lower, upper, anomaly\n",
    "\n",
    "    \n",
    "def format_message(time, predicted, lower, upper, anomaly):\n",
    "    message = \"\"\"\n",
    "    {\n",
    "      \"creation_time\": \\\"\"\"\"+time+\"\"\"\\\",\n",
    "      \"predicted\": \"\"\"+\"{:.8f}\".format(predicted)+\"\"\",\n",
    "      \"lower\": \"\"\"+\"{:.8f}\".format(lower)+\"\"\",\n",
    "      \"upper\": \"\"\"+\"{:.8f}\".format(upper)+\"\"\",\n",
    "      \"anomaly\": \"\"\"+\"{:.0f}\".format(anomaly)+\"\"\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    return message\n",
    "\n",
    "def send_to_topic(topic, message):\n",
    "    data = message.encode(\"utf-8\")\n",
    "    future = publisher.publish(topic, data)\n",
    "\n",
    "def publish_message(topic, time, predicted, lower, upper, anomaly, print_msg=False):\n",
    "    message = format_message(time, predicted, lower, upper, anomaly)\n",
    "    if print_msg:\n",
    "        print(message)\n",
    "    send_to_topic(topic, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    {\n",
      "      \"creation_time\": \"2022-07-15 17:08:25\",\n",
      "      \"predicted\": 0.59972642,\n",
      "      \"lower\": -1.40027358,\n",
      "      \"upper\": 2.59972642,\n",
      "      \"anomaly\": 1\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# SEND CURRENT TIMESTAMP\n",
    "\n",
    "from datetime import datetime\n",
    "time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "treshhold = 2\n",
    "actual = 2.5\n",
    "\n",
    "predicted = MODEL.forecast()[0]\n",
    "lower, upper, anomaly = check_anomaly(actual, predicted, treshhold)\n",
    "publish_message(TOPIC_2_URI, time, predicted, lower, upper, anomaly, print_msg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1506601594.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1/1506601594.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    BREAK HERE!!!!\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "BREAK HERE!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DEMO CLEAN UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DELETE - PUB SUB TO EMAIL TOPIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC_NAME = [your-topic]\n",
    "! gcloud pubsub topics delete $TOPIC_2_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DELETE - CLOUD FUNCTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC_NAME = [your-topic]\n",
    "####$$TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DELETE - LOCAL MODEL PKL FILE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# model_file = 'training/models/model.pkl'\n",
    "os.remove(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1506601594.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1/1506601594.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    BREAK HERE!!!!\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "BREAK HERE!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: SEND PREDICTIONS TO BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pub/Sub to BQ Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = '[your-topic]'\n",
    "TOPIC_URI = f'projects/{PROJECT_ID}/topics/{TOPIC_NAME}'\n",
    "TOPIC_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $TOPIC_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset in BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = '[your-dataset]'\n",
    "TABLE_NAME = f'{DATASET_NAME}.forecast_view'\n",
    "DATASET_URI = f'{PROJECT_ID}:{TABLE_NAME}'\n",
    "DATASET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bq mk --location=$REGION $DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bq mk $TABLE_NAME creation_time:TIMESTAMP,predicted:DECIMAL,lower:DECIMAL,upper:DECIMAL,anomaly:INTEGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIEW DATASET:\n",
    "! bq ls --format prettyjson $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DataFlow Pub/Sub to BQ streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFlow Pub/Sub to BQ pipeline:\n",
    "JOB_NAME = '[your-dataflow]'\n",
    "TEMPLATE = 'gs://dataflow-templates/latest/PubSub_to_BigQuery'\n",
    "STAGING = f'gs://{BUCKET_NAME}/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataflow jobs run hack22-dataflow     --gcs-location gs://dataflow-templates/latest/PubSub_to_BigQuery     --region $REGION     --staging-location $STAGING     --parameters inputTopic=$TOPIC_URI,outputTableSpec=$DATASET_URI,outputDeadletterTable=$DATASET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD TEST DATA BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BUCKET = f'{BUCKET_URI}/training'\n",
    "TEST_PREDICTIONS = f'{TRAINING_BUCKET}/data/test_predictions.csv' # FROM TRAINING NOTEBOOK - cell 59 - my4_Autoregressive_and_Automated_Methods_forTS.ipynb\n",
    "\n",
    "!gsutil cp gs://[your-bucket]/training/data/test_predictions.csv ./training/data/test_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   timestamp  46 non-null     object \n",
      " 1   predicted  46 non-null     float64\n",
      " 2   lower      46 non-null     float64\n",
      " 3   upper      46 non-null     float64\n",
      " 4   anomaly    46 non-null     int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 1.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_location = 'training/data/test_predictions.csv'\n",
    "test_df = pd.read_csv(file_location)\n",
    "test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>predicted</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-12-30 00:00:00</td>\n",
       "      <td>0.415759</td>\n",
       "      <td>-1.670546</td>\n",
       "      <td>2.329454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-12-30 01:00:00</td>\n",
       "      <td>0.339827</td>\n",
       "      <td>-1.709937</td>\n",
       "      <td>2.290063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-12-30 02:00:00</td>\n",
       "      <td>0.302746</td>\n",
       "      <td>-1.726052</td>\n",
       "      <td>2.273948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-30 03:00:00</td>\n",
       "      <td>0.287608</td>\n",
       "      <td>-1.731871</td>\n",
       "      <td>2.268129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-12-30 04:00:00</td>\n",
       "      <td>0.282120</td>\n",
       "      <td>-1.697404</td>\n",
       "      <td>2.302596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  predicted     lower     upper  anomaly\n",
       "0  2014-12-30 00:00:00   0.415759 -1.670546  2.329454        0\n",
       "1  2014-12-30 01:00:00   0.339827 -1.709937  2.290063        0\n",
       "2  2014-12-30 02:00:00   0.302746 -1.726052  2.273948        0\n",
       "3  2014-12-30 03:00:00   0.287608 -1.731871  2.268129        0\n",
       "4  2014-12-30 04:00:00   0.282120 -1.697404  2.302596        0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID                                    NAME                           TYPE       CREATION_TIME        STATE    REGION\n",
      "2022-07-15_10_19_59-16597450199510106227  hack22-dataflow                Streaming  2022-07-15 17:19:59  Running  us-central1\n",
      "2022-07-15_08_21_06-16100316668033096407  ps-to-text-hack22-email-topic  Streaming  2022-07-15 15:21:06  Running  us-central1\n"
     ]
    }
   ],
   "source": [
    "# Check if Dataflow jobs are running:\n",
    "! gcloud dataflow jobs list --region=$REGION --status='active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BATCH TO BQ\n",
    "\n",
    "from google.cloud import pubsub_v1\n",
    "from joblib import load\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "print('TOPIC_URI: '+TOPIC_URI)\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    time = row['timestamp']\n",
    "    predicted = row['predicted']\n",
    "    lower = row['lower']\n",
    "    upper = row['upper']\n",
    "    anomaly = row['anomaly']\n",
    "    publish_message(TOPIC_URI, time, predicted, lower, upper, anomaly, print_msg=False)\n",
    "print('Published all messages') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+-------------+------------+---------+\n",
      "|    creation_time    | predicted  |    lower    |   upper    | anomaly |\n",
      "+---------------------+------------+-------------+------------+---------+\n",
      "| 2014-12-31 21:00:00 | 0.69218379 | -1.38048344 | 2.61951656 |       0 |\n",
      "| 2014-12-31 20:00:00 | 0.75800251 | -1.29767234 | 2.70232766 |       0 |\n",
      "| 2014-12-31 19:00:00 | 0.83715234 | -1.22784244 | 2.77215756 |       0 |\n",
      "| 2014-12-31 18:00:00 | 0.88792316 | -1.14413608 | 2.85586392 |       0 |\n",
      "| 2014-12-31 17:00:00 | 0.81233316 | -1.09042077 | 2.90957923 |       0 |\n",
      "| 2014-12-31 16:00:00 |  0.7155543 | -1.17009848 | 2.82990152 |       0 |\n",
      "| 2014-12-31 15:00:00 | 0.70769909 | -1.27260519 | 2.72739481 |       0 |\n",
      "| 2014-12-31 14:00:00 | 0.72464297 | -1.28111012 | 2.71888988 |       0 |\n",
      "| 2014-12-31 13:00:00 | 0.74536472 | -1.26320501 | 2.73679499 |       0 |\n",
      "| 2014-12-31 12:00:00 | 0.76567399 | -1.24127126 | 2.75872874 |       0 |\n",
      "+---------------------+------------+-------------+------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF MESSAGES MADE IT TO BQ:\n",
    "!bq query --use_legacy_sql=false 'SELECT * FROM `'\"[your-project-id].[your-dataset].forecast_view\"'`ORDER BY creation_time desc limit 10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEMO STREAMING TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "\n",
    "MODEL_LOCATION = f\"{BUCKET_URI}/training/models/energy.pkl\"\n",
    "model_file = 'training/models/model.pkl'\n",
    "! gsutil cp -r $MODEL_LOCATION $model_file\n",
    "\n",
    "from joblib import load\n",
    "MODEL = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    {\n",
      "      \"creation_time\": \"2014-12-31 21:00:00\",\n",
      "      \"predicted\": 0.59972642,\n",
      "      \"lower\": -1.40027358,\n",
      "      \"upper\": 2.59972642,\n",
      "      \"anomaly\": 1\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# SEND ANOMALY EXAMPLE\n",
    "\n",
    "time = \"2014-12-31 21:00:00\"\n",
    "treshhold = 2\n",
    "actual = 24\n",
    "predicted = MODEL.forecast()[0]\n",
    "lower, upper, anomaly = check_anomaly(actual, predicted, treshhold)\n",
    "publish_message(TOPIC_URI, time, predicted, lower, upper, anomaly, print_msg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    {\n",
      "      \"creation_time\": \"2014-12-31 22:00:00\",\n",
      "      \"predicted\": 0.59972642,\n",
      "      \"lower\": -1.40027358,\n",
      "      \"upper\": 2.59972642,\n",
      "      \"anomaly\": 1\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# SEND REGULAR EXAMPLE\n",
    "\n",
    "time = \"2014-12-31 22:00:00\"\n",
    "treshhold = 2\n",
    "actual = 3\n",
    "predicted = MODEL.forecast()[0]\n",
    "lower, upper, anomaly = check_anomaly(actual, predicted, treshhold)\n",
    "publish_message(TOPIC_URI, time, predicted, lower, upper, anomaly, print_msg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    {\n",
      "      \"creation_time\": \"2022-07-15 17:25:43\",\n",
      "      \"predicted\": 0.59972642,\n",
      "      \"lower\": -1.40027358,\n",
      "      \"upper\": 2.59972642,\n",
      "      \"anomaly\": 1\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# SEND CURRENT TIMESTAMP\n",
    "\n",
    "from datetime import datetime\n",
    "time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "treshhold = 2\n",
    "actual = 2.5\n",
    "predicted = MODEL.forecast()[0]\n",
    "lower, upper, anomaly = check_anomaly(actual, predicted, treshhold)\n",
    "publish_message(TOPIC_URI, time, predicted, lower, upper, anomaly, print_msg=True) #@param TOPIC_URI string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0715 17:25:47.312982572    1275 backup_poller.cc:136]       Run client channel backup poller: {\"created\":\"@1657905947.312882234\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epoll1_linux.cc\",\"file_line\":247,\"referenced_errors\":[{\"created\":\"@1657905947.312874766\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epoll1_linux.cc\",\"file_line\":732,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "+---------------------+------------+-------------+------------+---------+\n",
      "|    creation_time    | predicted  |    lower    |   upper    | anomaly |\n",
      "+---------------------+------------+-------------+------------+---------+\n",
      "| 2022-07-15 17:25:43 | 0.59972642 | -1.40027358 | 2.59972642 |       1 |\n",
      "| 2014-12-31 22:00:00 | 0.59972642 | -1.40027358 | 2.59972642 |       1 |\n",
      "| 2014-12-31 21:00:00 | 0.59972642 | -1.40027358 | 2.59972642 |       1 |\n",
      "| 2014-12-31 21:00:00 | 0.69218379 | -1.38048344 | 2.61951656 |       0 |\n",
      "| 2014-12-31 20:00:00 | 0.75800251 | -1.29767234 | 2.70232766 |       0 |\n",
      "| 2014-12-31 19:00:00 | 0.83715234 | -1.22784244 | 2.77215756 |       0 |\n",
      "| 2014-12-31 18:00:00 | 0.88792316 | -1.14413608 | 2.85586392 |       0 |\n",
      "| 2014-12-31 17:00:00 | 0.81233316 | -1.09042077 | 2.90957923 |       0 |\n",
      "| 2014-12-31 16:00:00 |  0.7155543 | -1.17009848 | 2.82990152 |       0 |\n",
      "| 2014-12-31 15:00:00 | 0.70769909 | -1.27260519 | 2.72739481 |       0 |\n",
      "+---------------------+------------+-------------+------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF MESSAGES MADE IT TO BQ:\n",
    "!bq query --use_legacy_sql=false 'SELECT * FROM `'\"[your-project-id].hack22_dataset.forecast_view\"'`ORDER BY creation_time desc limit 10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### STOP -  DATAFLOW JOB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID                                    NAME                           TYPE       CREATION_TIME        STATE    REGION\n",
      "2022-07-15_10_19_59-16597450199510106227  hack22-dataflow                Streaming  2022-07-15 17:19:59  Running  us-central1\n",
      "2022-07-15_08_21_06-16100316668033096407  ps-to-text-hack22-email-topic  Streaming  2022-07-15 15:21:06  Running  us-central1\n"
     ]
    }
   ],
   "source": [
    "# Check if Dataflow jobs are running:\n",
    "! gcloud dataflow jobs list --region=$REGION --status='active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with the 'id' value from previous cell\n",
    "JOB_ID = '2022-07-15_08_21_06-16100316668033096407' #@param:string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelled job [2022-07-15_08_21_06-16100316668033096407]\n"
     ]
    }
   ],
   "source": [
    "#Stop the pipeline:\n",
    "! gcloud dataflow jobs cancel $JOB_ID --region=$REGION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DELETE - BQ DATASET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bq rm -r=true -f=true $DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK HERE!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp [your-nb]_deployment.ipynb $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUCKET_NAME = \"[your-bucket]\"\n",
    "# BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "# BUCKET_URI\n",
    "! gsutil cp -r functions $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths  \n",
    "\n",
    "def zip_files(directory):\n",
    "    # path to folder which needs to be zipped\n",
    "    # directory = './functions'\n",
    "  \n",
    "    # calling function to get all file paths in the directory\n",
    "    file_paths = get_all_file_paths(directory)\n",
    "  \n",
    "    # printing the list of all files to be zipped\n",
    "    print('Following files will be zipped:')\n",
    "    for file_name in file_paths:\n",
    "        print(file_name)\n",
    "  \n",
    "    # writing files to a zipfile\n",
    "    with ZipFile('functions.zip','w') as zip:\n",
    "        # writing each file one by one\n",
    "        for file in file_paths:\n",
    "            zip.write(file)\n",
    "  \n",
    "    print('All files zipped successfully!')\n",
    "    \n",
    "zip_files('./functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp -r 'functions.zip' gs://[your-bucket]/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://[your-bucket]/functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST PUBLISH TO THE TOPIC\n",
    "\n",
    "# # Test in the Pub/Sub UI and check if it worked\n",
    "# # UI link = https://console.cloud.google.com/cloudpubsub/topic/detail/[your-topic]?project=[your-project-id]&tab=messages\n",
    "\n",
    "# test_massage_1 = \"\"\"\n",
    "# {\n",
    "#   \"timestamp\": \"2022-07-14 12:11:35.22 \",\n",
    "#   \"predicted\": 30.23,\n",
    "#   \"lower\": 26.274,\n",
    "#   \"upper\": 32.573,\n",
    "#   \"anomaly\": \"N\"\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# !bq query --use_legacy_sql=false 'SELECT * FROM `'\"[your-project-id].hack22_dataset.forecast_view\"'`'\n",
    "\n",
    "# from google.cloud import pubsub_v1\n",
    "\n",
    "# publisher = pubsub_v1.PublisherClient()\n",
    "# time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# test_massage_2 = \"\"\"\n",
    "# {\n",
    "#   \"timestamp\": \\\"\"\"\"+time+\"\"\"\\\",\n",
    "#   \"predicted\": 0.522726416,\n",
    "#   \"lower\": -1.22227358,\n",
    "#   \"upper\": 2.11172641,\n",
    "#   \"anomaly\": \"Y\"\n",
    "# }\n",
    "# \"\"\"\n",
    "# data = test_massage_2.encode(\"utf-8\")\n",
    "# future = publisher.publish(TOPIC_URI, data)\n",
    "# print(future.result())\n",
    "\n",
    "# !bq query --use_legacy_sql=false 'SELECT * FROM `'\"[your-project-id].hack22_dataset.forecast_view\"'`'\n",
    "\n",
    "# predicted = MODEL.forecast()[0]\n",
    "# print(predicted)\n",
    "# treshhold = 2\n",
    "# upper = predicted+treshhold\n",
    "# lower = predicted-treshhold\n",
    "# time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# anomaly = \"Y\"\n",
    "\n",
    "# test_massage_3 = \"\"\"\n",
    "# {\n",
    "#   \"timestamp\": \\\"\"\"\"+time+\"\"\"\\\",\n",
    "#   \"predicted\": \"\"\"+\"{:.8f}\".format(predicted)+\"\"\",\n",
    "#   \"lower\": \"\"\"+\"{:.8f}\".format(lower)+\"\"\",\n",
    "#   \"upper\": \"\"\"+\"{:.8f}\".format(upper)+\"\"\",\n",
    "#   \"anomaly\": \\\"\"\"\"+anomaly+\"\"\"\\\"\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# print(test_massage_3)\n",
    "# data = test_massage_3.encode(\"utf-8\")\n",
    "# future = publisher.publish(TOPIC_URI, data)\n",
    "\n",
    "# !bq query --use_legacy_sql=false 'SELECT * FROM `'\"[your-project-id].hack22_dataset.forecast_view\"'`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND TEST MODEL\n",
    "\n",
    "# import os\n",
    "\n",
    "# MODEL_LOCATION = f\"{BUCKET_URI}/app_engine/models/model.pkl\"\n",
    "# model_file = 'model.pkl'\n",
    "# ! gsutil cp -r $MODEL_LOCATION $model_file\n",
    "\n",
    "# from joblib import load\n",
    "\n",
    "# MODEL = load(model_file)\n",
    "# MODEL.forecast()\n",
    "# MODEL.forecast(steps=3)\n",
    "\n",
    "# os.remove(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import pubsub_v1\n",
    "\n",
    "# publisher = pubsub_v1.PublisherClient()\n",
    "# TOPIC_PATH = publisher.topic_path(PROJECT_ID, TOPIC_ID)\n",
    "# print(TOPIC_PATH)\n",
    "# # topic = publisher.create_topic(request={\"name\": TOPIC_PATH})\n",
    "# topic = publisher.create_topic(TOPIC_PATH)\n",
    "# print(f\"Created topic: {topic.name}\")\n",
    "\n",
    "# # publisher.delete_topic(TOPIC_PATH)\n",
    "# # print(f\"Topic deleted: {topic.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# bq cp myDataset.myTable myDataset.myTableCopy\n",
    "# bq extract --compression=GZIP --destination_format=CSV --field_delimiter=tab --print_header=false myDataset.myTable gs://my-bucket/myFile.csv.gzip\n",
    "# bq head --max_rows=10 --start_row=50 --selected_fields=field1,field3 myDataset.myTable\n",
    "# bq insert --ignore_unknown_values --template_suffix=_insert myDataset.myTable /tmp/myData.json\n",
    "# bq ls myDataset\n",
    "# !bq query --use_legacy_sql=false 'SELECT * FROM `'\"[your-project-id].hack22_dataset.forecast_view\"'`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTARTING THE PIPELINE\n",
    "# There is no possiblity of restarting existing pipeline as it is not supported. To restart simply run the job run cell again\n",
    "\n",
    "# NOTES:\n",
    "# Create DataFlow Pub/Sub to BQ pipeline:\n",
    "# !gcloud dataflow jobs run hack22-dataflow     --gcs-location gs://dataflow-templates/latest/PubSub_to_BigQuery     --region $REGION     --staging-location gs://[your-bucket]/temp     --parameters inputTopic=projects/[your-project-id]/topics/hack22-topic,outputTableSpec=[your-project-id]:hack22_dataset.forecast_view,outputDeadletterTable=[your-project-id]:hack22_dataset.forecast_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "## Service Account\n",
    "\n",
    "# **If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP_LOCATION = \"{}/app_engine\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "# SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "# if (\n",
    "#     SERVICE_ACCOUNT == \"\"\n",
    "#     or SERVICE_ACCOUNT is None\n",
    "#     or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "# ):\n",
    "#     # Get your service account from gcloud\n",
    "\n",
    "#     shell_output = !gcloud auth list 2>/dev/null\n",
    "#     SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "#     print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "# Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
